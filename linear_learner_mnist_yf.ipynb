{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Linear Learner with MNIST\n",
    "_**Making a Binary Prediction of Whether a Handwritten Digit is a 0**_\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "  2. [Data ingestion](#Data-ingestion)\n",
    "  3. [Data inspection](#Data-inspection)\n",
    "  4. [Data conversion](#Data-conversion)\n",
    "3. [Training the linear model](#Training-the-linear-model)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "5. [Validate the model for use](#Validate-the-model-for-use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to our example introducing Amazon SageMaker's Linear Learner Algorithm!  Today, we're analyzing the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset which consists of images of handwritten digits, from zero to nine.  We'll use the individual pixel values from each 28 x 28 grayscale image to predict a yes or no label of whether the digit is a 0 or some other digit (1, 2, 3, ... 9).\n",
    "\n",
    "The method that we'll use is a linear binary classifier.  Linear models are supervised learning algorithms used for solving either classification or regression problems.  As input, the model is given labeled examples ( **`x`**, `y`). **`x`** is a high dimensional vector and `y` is a numeric label.  Since we are doing binary classification, the algorithm expects the label to be either 0 or 1 (but Amazon SageMaker Linear Learner also supports regression on continuous values of `y`).  The algorithm learns a linear function, or linear threshold function for classification, mapping the vector **`x`** to an approximation of the label `y`.\n",
    "\n",
    "Amazon SageMaker's Linear Learner algorithm extends upon typical linear models by training many models in parallel, in a computationally efficient manner.  Each model has a different set of hyperparameters, and then the algorithm finds the set that optimizes a specific criteria.  This can provide substantially more accurate models than typical linear algorithms at the same, or lower, cost.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "prefix = 'sagemaker/DEMO-linear-mnist'\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>name</th>\n",
       "      <th>numFriends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>Will</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>Jean-Luc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>Hugh</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>Deanna</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>Quark</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  age      name  numFriends\n",
       "0   0   33      Will         385\n",
       "1   1   26  Jean-Luc           2\n",
       "2   2   55      Hugh         221\n",
       "3   3   40    Deanna         465\n",
       "4   4   68     Quark          21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "import pandas as pd\n",
    "bucket='yifenghe2019'\n",
    "data_key = 'friends.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "df = pd.read_csv(data_location)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-15\n",
      "2018-08-15/\n",
      "2018-08-15/2018-08-15_BINS_XETR00.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR01.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR02.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR03.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR04.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR05.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR06.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR07.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR08.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR09.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR10.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR11.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR12.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR13.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR14.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR15.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR16.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR17.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR18.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR19.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR20.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR21.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR22.csv\n",
      "2018-08-15/2018-08-15_BINS_XETR23.csv\n",
      "data/\n",
      "data/11.txt\n",
      "data/friends.csv\n",
      "friends.csv\n",
      "friends2.csv\n",
      "input_json.txt\n",
      "input_step_functions.json\n",
      "parquet/\n",
      "parquet/part-00000-2205e3f1-495d-4854-bbc4-57c27f27ca22-c000.snappy.parquet\n",
      "parquet_$folder$\n",
      "pk/\n",
      "pk/friends.csv\n",
      "pk_$folder$\n"
     ]
    }
   ],
   "source": [
    "# list files\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('yifenghe2019')\n",
    "for obj in bucket.objects.all():\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[s3.ObjectSummary(bucket_name='yifenghe2019', key='data/'),\n",
       " s3.ObjectSummary(bucket_name='yifenghe2019', key='data/11.txt'),\n",
       " s3.ObjectSummary(bucket_name='yifenghe2019', key='data/friends.csv')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To list all the files in the folder path/to/my/folder in my-bucket:\n",
    "files = list(bucket.objects.filter(Prefix='data/'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj3 = files[2].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '2EB5C8B7D82D9BD0',\n",
       "  'HostId': 'FrzMQ2iR+0HYOx5JdIETNzW7Fh0juoDoy+ZNE+sWbcldsjI2wdrKuh66Z1QvcuBYh9qB+iDxgMg=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'FrzMQ2iR+0HYOx5JdIETNzW7Fh0juoDoy+ZNE+sWbcldsjI2wdrKuh66Z1QvcuBYh9qB+iDxgMg=',\n",
       "   'x-amz-request-id': '2EB5C8B7D82D9BD0',\n",
       "   'date': 'Wed, 27 Feb 2019 21:30:40 GMT',\n",
       "   'last-modified': 'Wed, 27 Feb 2019 20:54:37 GMT',\n",
       "   'etag': '\"340f94f5263e12049766b55141dc7ee6\"',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'text/csv',\n",
       "   'content-length': '8277',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2019, 2, 27, 20, 54, 37, tzinfo=tzutc()),\n",
       " 'ContentLength': 8277,\n",
       " 'ETag': '\"340f94f5263e12049766b55141dc7ee6\"',\n",
       " 'ContentType': 'text/csv',\n",
       " 'Metadata': {},\n",
       " 'Body': <botocore.response.StreamingBody at 0x7f207851e860>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_csv(obj3['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'a': 1, 'b': 2}\n",
      "\n",
      "ID,age,name,numFriends\n",
      "0,33,Will,385\n",
      "1,26,Jean-Luc,2\n",
      "2,55,Hugh,221\n",
      "3,40,Deanna,465\n",
      "4,68,Quark,21\n",
      "5,59,Weyoun,318\n",
      "6,37,Gowron,220\n",
      "7,54,Will,307\n",
      "8,38,Jadzia,380\n",
      "9,27,Hugh,181\n",
      "10,53,Odo,191\n",
      "11,57,Ben,372\n",
      "12,54,Keiko,253\n",
      "13,56,Jean-Luc,444\n",
      "14,43,Hugh,49\n",
      "15,36,Rom,49\n",
      "16,22,Weyoun,323\n",
      "17,35,Odo,13\n",
      "18,45,Jean-Luc,455\n",
      "19,60,Geordi,246\n",
      "20,67,Odo,220\n",
      "21,19,Miles,268\n",
      "22,30,Quark,72\n",
      "23,51,Keiko,271\n",
      "24,25,Julian,1\n",
      "25,21,Ben,445\n",
      "26,22,Julian,100\n",
      "27,42,Leeta,363\n",
      "28,49,Martok,476\n",
      "29,48,Nog,364\n",
      "30,50,Keiko,175\n",
      "31,39,Miles,161\n",
      "32,26,Nog,281\n",
      "33,53,Dukat,197\n",
      "34,43,Jean-Luc,249\n",
      "35,27,Beverly,305\n",
      "36,32,Kasidy,81\n",
      "37,58,Geordi,21\n",
      "38,64,Deanna,65\n",
      "39,31,Morn,192\n",
      "40,52,Odo,413\n",
      "41,67,Hugh,167\n",
      "42,54,Brunt,75\n",
      "43,58,Guinan,345\n",
      "44,35,Nerys,244\n",
      "45,52,Dukat,77\n",
      "46,25,Morn,96\n",
      "47,24,Brunt,49\n",
      "48,20,Nog,1\n",
      "49,40,Ezri,254\n",
      "50,51,Quark,283\n",
      "51,36,Lwaxana,212\n",
      "52,19,Beverly,269\n",
      "53,62,Geordi,31\n",
      "54,19,Brunt,5\n",
      "55,41,Keiko,278\n",
      "56,44,Gowron,194\n",
      "57,57,Odo,294\n",
      "58,59,Hugh,158\n",
      "59,59,Morn,284\n",
      "60,20,Geordi,100\n",
      "61,62,Kasidy,442\n",
      "62,69,Keiko,9\n",
      "63,58,Jean-Luc,54\n",
      "64,31,Elim,15\n",
      "65,52,Guinan,169\n",
      "66,21,Geordi,477\n",
      "67,48,Jadzia,135\n",
      "68,33,Guinan,74\n",
      "69,30,Jean-Luc,204\n",
      "70,52,Brunt,393\n",
      "71,45,Geordi,184\n",
      "72,22,Kasidy,179\n",
      "73,20,Brunt,384\n",
      "74,65,Leeta,208\n",
      "75,40,Morn,459\n",
      "76,62,Will,201\n",
      "77,40,Weyoun,407\n",
      "78,61,Data,337\n",
      "79,58,Leeta,348\n",
      "80,67,Dukat,445\n",
      "81,54,Jadzia,440\n",
      "82,57,Hugh,465\n",
      "83,32,Geordi,308\n",
      "84,28,Ben,311\n",
      "85,66,Quark,383\n",
      "86,55,Hugh,257\n",
      "87,31,Ezri,481\n",
      "88,66,Ben,188\n",
      "89,24,Worf,492\n",
      "90,33,Kasidy,471\n",
      "91,46,Rom,88\n",
      "92,54,Gowron,7\n",
      "93,46,Elim,63\n",
      "94,62,Morn,133\n",
      "95,29,Odo,173\n",
      "96,25,Ezri,233\n",
      "97,69,Nerys,361\n",
      "98,44,Will,178\n",
      "99,69,Keiko,491\n",
      "100,61,Jean-Luc,460\n",
      "101,67,Morn,123\n",
      "102,40,Dukat,18\n",
      "103,61,Ezri,2\n",
      "104,32,Dukat,142\n",
      "105,50,Morn,417\n",
      "106,18,Beverly,499\n",
      "107,64,Will,419\n",
      "108,25,Leeta,274\n",
      "109,53,Quark,417\n",
      "110,64,Nog,137\n",
      "111,37,Nerys,46\n",
      "112,25,Morn,13\n",
      "113,41,Quark,244\n",
      "114,33,Worf,275\n",
      "115,18,Dukat,397\n",
      "116,69,Ben,75\n",
      "117,52,Rom,487\n",
      "118,28,Ben,304\n",
      "119,29,Worf,344\n",
      "120,68,Jean-Luc,264\n",
      "121,35,Deanna,355\n",
      "122,45,Data,400\n",
      "123,45,Jadzia,439\n",
      "124,47,Data,429\n",
      "125,66,Rom,284\n",
      "126,26,Brunt,84\n",
      "127,40,Miles,284\n",
      "128,34,Julian,221\n",
      "129,45,Kasidy,252\n",
      "130,67,Gowron,350\n",
      "131,65,Hugh,309\n",
      "132,46,Odo,462\n",
      "133,19,Quark,265\n",
      "134,45,Ben,340\n",
      "135,42,Rom,427\n",
      "136,19,Will,335\n",
      "137,28,Martok,32\n",
      "138,32,Dukat,384\n",
      "139,36,Nog,193\n",
      "140,64,Elim,234\n",
      "141,36,Miles,424\n",
      "142,59,Guinan,335\n",
      "143,60,Data,124\n",
      "144,22,Miles,93\n",
      "145,45,Leeta,470\n",
      "146,58,Nerys,174\n",
      "147,61,Quark,373\n",
      "148,39,Nerys,248\n",
      "149,49,Beverly,340\n",
      "150,55,Nerys,313\n",
      "151,54,Keiko,441\n",
      "152,54,Kasidy,235\n",
      "153,63,Morn,342\n",
      "154,40,Geordi,389\n",
      "155,50,Beverly,126\n",
      "156,44,Deanna,360\n",
      "157,34,Dukat,319\n",
      "158,31,Odo,340\n",
      "159,67,Kasidy,438\n",
      "160,58,Beverly,112\n",
      "161,39,Odo,207\n",
      "162,59,Ezri,14\n",
      "163,67,Nerys,204\n",
      "164,31,Will,172\n",
      "165,26,Leeta,282\n",
      "166,25,Lwaxana,10\n",
      "167,48,Quark,57\n",
      "168,68,Martok,112\n",
      "169,53,Beverly,92\n",
      "170,68,Jean-Luc,490\n",
      "171,29,Weyoun,126\n",
      "172,55,Kasidy,204\n",
      "173,23,Leeta,129\n",
      "174,47,Deanna,87\n",
      "175,38,Will,459\n",
      "176,55,Worf,474\n",
      "177,67,Brunt,316\n",
      "178,26,Kasidy,381\n",
      "179,37,Elim,426\n",
      "180,30,Kasidy,108\n",
      "181,43,Rom,404\n",
      "182,26,Weyoun,145\n",
      "183,47,Ben,488\n",
      "184,44,Julian,84\n",
      "185,48,Weyoun,287\n",
      "186,31,Miles,109\n",
      "187,47,Nerys,225\n",
      "188,54,Keiko,369\n",
      "189,62,Quark,23\n",
      "190,60,Geordi,294\n",
      "191,40,Nog,349\n",
      "192,45,Jadzia,497\n",
      "193,60,Nerys,125\n",
      "194,38,Kasidy,2\n",
      "195,30,Ben,376\n",
      "196,38,Data,173\n",
      "197,38,Leeta,76\n",
      "198,48,Brunt,381\n",
      "199,38,Hugh,180\n",
      "200,21,Kasidy,472\n",
      "201,23,Ezri,174\n",
      "202,63,Lwaxana,469\n",
      "203,46,Ezri,125\n",
      "204,64,Deanna,164\n",
      "205,69,Morn,236\n",
      "206,21,Will,491\n",
      "207,41,Lwaxana,206\n",
      "208,37,Nog,271\n",
      "209,27,Brunt,174\n",
      "210,33,Data,245\n",
      "211,61,Ben,73\n",
      "212,55,Geordi,284\n",
      "213,28,Worf,312\n",
      "214,32,Miles,182\n",
      "215,22,Will,6\n",
      "216,34,Brunt,116\n",
      "217,29,Keiko,260\n",
      "218,66,Gowron,350\n",
      "219,26,Lwaxana,345\n",
      "220,41,Jean-Luc,394\n",
      "221,27,Dukat,150\n",
      "222,34,Rom,346\n",
      "223,40,Odo,406\n",
      "224,44,Keiko,277\n",
      "225,19,Elim,106\n",
      "226,37,Lwaxana,207\n",
      "227,40,Ezri,198\n",
      "228,26,Martok,293\n",
      "229,24,Gowron,150\n",
      "230,54,Beverly,397\n",
      "231,59,Ezri,42\n",
      "232,68,Worf,481\n",
      "233,67,Gowron,70\n",
      "234,49,Deanna,22\n",
      "235,57,Elim,8\n",
      "236,62,Brunt,442\n",
      "237,61,Nerys,469\n",
      "238,25,Deanna,305\n",
      "239,48,Nog,345\n",
      "240,46,Deanna,154\n",
      "241,45,Quark,332\n",
      "242,25,Data,101\n",
      "243,61,Martok,68\n",
      "244,21,Dukat,471\n",
      "245,28,Jean-Luc,174\n",
      "246,41,Leeta,260\n",
      "247,52,Ezri,338\n",
      "248,21,Dukat,138\n",
      "249,66,Nerys,41\n",
      "250,36,Hugh,342\n",
      "251,55,Rom,57\n",
      "252,36,Will,174\n",
      "253,69,Leeta,116\n",
      "254,67,Ezri,79\n",
      "255,60,Deanna,324\n",
      "256,32,Worf,412\n",
      "257,51,Data,161\n",
      "258,68,Worf,217\n",
      "259,29,Kasidy,11\n",
      "260,38,Brunt,96\n",
      "261,40,Jadzia,172\n",
      "262,51,Will,334\n",
      "263,40,Martok,33\n",
      "264,29,Julian,228\n",
      "265,27,Gowron,471\n",
      "266,66,Jean-Luc,496\n",
      "267,49,Dukat,106\n",
      "268,26,Ezri,298\n",
      "269,55,Beverly,289\n",
      "270,44,Data,353\n",
      "271,25,Morn,446\n",
      "272,29,Quark,367\n",
      "273,51,Data,493\n",
      "274,64,Julian,244\n",
      "275,47,Will,13\n",
      "276,54,Dukat,462\n",
      "277,46,Hugh,300\n",
      "278,44,Data,499\n",
      "279,23,Beverly,133\n",
      "280,26,Nerys,492\n",
      "281,21,Worf,89\n",
      "282,32,Geordi,404\n",
      "283,65,Dukat,443\n",
      "284,26,Nog,269\n",
      "285,43,Data,101\n",
      "286,30,Lwaxana,384\n",
      "287,64,Beverly,396\n",
      "288,56,Hugh,354\n",
      "289,30,Ezri,221\n",
      "290,62,Beverly,290\n",
      "291,23,Dukat,373\n",
      "292,63,Nog,380\n",
      "293,23,Deanna,65\n",
      "294,38,Leeta,410\n",
      "295,40,Nerys,56\n",
      "296,38,Data,454\n",
      "297,45,Ben,395\n",
      "298,57,Guinan,207\n",
      "299,57,Rom,311\n",
      "300,49,Beverly,147\n",
      "301,28,Weyoun,108\n",
      "302,37,Beverly,263\n",
      "303,46,Deanna,319\n",
      "304,19,Will,404\n",
      "305,29,Quark,182\n",
      "306,23,Beverly,323\n",
      "307,41,Keiko,340\n",
      "308,45,Morn,59\n",
      "309,67,Geordi,153\n",
      "310,68,Odo,189\n",
      "311,43,Martok,48\n",
      "312,61,Jadzia,421\n",
      "313,59,Dukat,169\n",
      "314,36,Geordi,168\n",
      "315,25,Weyoun,208\n",
      "316,64,Hugh,391\n",
      "317,59,Guinan,439\n",
      "318,35,Deanna,251\n",
      "319,30,Leeta,476\n",
      "320,62,Worf,450\n",
      "321,44,Data,61\n",
      "322,58,Rom,92\n",
      "323,29,Nog,236\n",
      "324,56,Miles,343\n",
      "325,51,Keiko,492\n",
      "326,46,Beverly,407\n",
      "327,20,Julian,63\n",
      "328,62,Deanna,41\n",
      "329,67,Dukat,35\n",
      "330,33,Ezri,356\n",
      "331,30,Martok,17\n",
      "332,55,Julian,362\n",
      "333,29,Ben,207\n",
      "334,40,Leeta,7\n",
      "335,27,Odo,337\n",
      "336,47,Gowron,4\n",
      "337,58,Miles,10\n",
      "338,28,Will,180\n",
      "339,66,Morn,305\n",
      "340,57,Nerys,275\n",
      "341,18,Data,326\n",
      "342,46,Guinan,151\n",
      "343,26,Odo,254\n",
      "344,30,Data,487\n",
      "345,31,Ezri,394\n",
      "346,29,Hugh,329\n",
      "347,32,Geordi,24\n",
      "348,33,Weyoun,460\n",
      "349,20,Kasidy,277\n",
      "350,55,Nog,464\n",
      "351,54,Keiko,72\n",
      "352,27,Deanna,53\n",
      "353,64,Julian,499\n",
      "354,69,Kasidy,15\n",
      "355,46,Keiko,352\n",
      "356,67,Weyoun,149\n",
      "357,26,Brunt,7\n",
      "358,52,Will,276\n",
      "359,54,Nog,442\n",
      "360,39,Nerys,68\n",
      "361,68,Worf,206\n",
      "362,39,Ezri,120\n",
      "363,41,Dukat,397\n",
      "364,54,Lwaxana,115\n",
      "365,65,Brunt,430\n",
      "366,19,Keiko,119\n",
      "367,39,Data,106\n",
      "368,26,Elim,383\n",
      "369,48,Quark,266\n",
      "370,53,Jadzia,86\n",
      "371,31,Guinan,435\n",
      "372,62,Brunt,273\n",
      "373,19,Quark,272\n",
      "374,68,Nog,293\n",
      "375,66,Hugh,201\n",
      "376,23,Gowron,392\n",
      "377,18,Beverly,418\n",
      "378,47,Guinan,97\n",
      "379,60,Data,304\n",
      "380,35,Brunt,65\n",
      "381,38,Nog,95\n",
      "382,66,Worf,240\n",
      "383,69,Data,148\n",
      "384,67,Martok,355\n",
      "385,57,Beverly,436\n",
      "386,35,Data,428\n",
      "387,43,Will,335\n",
      "388,30,Nog,184\n",
      "389,38,Weyoun,38\n",
      "390,22,Martok,266\n",
      "391,64,Ben,309\n",
      "392,64,Data,343\n",
      "393,50,Quark,436\n",
      "394,23,Keiko,230\n",
      "395,56,Jean-Luc,15\n",
      "396,67,Keiko,38\n",
      "397,69,Quark,470\n",
      "398,26,Lwaxana,124\n",
      "399,24,Beverly,401\n",
      "400,29,Data,128\n",
      "401,42,Jean-Luc,467\n",
      "402,58,Hugh,98\n",
      "403,21,Weyoun,224\n",
      "404,18,Kasidy,24\n",
      "405,56,Nog,371\n",
      "406,57,Ben,121\n",
      "407,36,Miles,68\n",
      "408,62,Dukat,496\n",
      "409,19,Nog,267\n",
      "410,35,Odo,299\n",
      "411,58,Lwaxana,22\n",
      "412,53,Jadzia,451\n",
      "413,45,Hugh,147\n",
      "414,56,Martok,313\n",
      "415,30,Quark,65\n",
      "416,33,Nerys,294\n",
      "417,37,Julian,106\n",
      "418,32,Guinan,212\n",
      "419,55,Kasidy,176\n",
      "420,26,Jadzia,391\n",
      "421,40,Will,261\n",
      "422,67,Ben,292\n",
      "423,44,Will,388\n",
      "424,55,Keiko,470\n",
      "425,33,Quark,243\n",
      "426,24,Worf,77\n",
      "427,28,Brunt,258\n",
      "428,68,Lwaxana,423\n",
      "429,63,Jean-Luc,345\n",
      "430,36,Geordi,493\n",
      "431,36,Quark,343\n",
      "432,45,Brunt,54\n",
      "433,38,Ezri,203\n",
      "434,57,Deanna,289\n",
      "435,42,Guinan,275\n",
      "436,57,Geordi,229\n",
      "437,59,Morn,221\n",
      "438,42,Nog,95\n",
      "439,18,Data,417\n",
      "440,48,Elim,394\n",
      "441,38,Jadzia,143\n",
      "442,46,Nog,105\n",
      "443,64,Geordi,175\n",
      "444,18,Keiko,472\n",
      "445,40,Guinan,286\n",
      "446,32,Quark,41\n",
      "447,38,Julian,34\n",
      "448,48,Nerys,439\n",
      "449,52,Data,419\n",
      "450,37,Weyoun,234\n",
      "451,28,Martok,34\n",
      "452,58,Ezri,6\n",
      "453,44,Julian,337\n",
      "454,52,Weyoun,456\n",
      "455,33,Elim,463\n",
      "456,37,Ezri,471\n",
      "457,51,Worf,81\n",
      "458,44,Elim,335\n",
      "459,26,Geordi,84\n",
      "460,47,Hugh,400\n",
      "461,41,Geordi,236\n",
      "462,23,Nerys,287\n",
      "463,40,Keiko,220\n",
      "464,25,Beverly,485\n",
      "465,53,Morn,126\n",
      "466,33,Brunt,228\n",
      "467,42,Weyoun,194\n",
      "468,46,Ezri,227\n",
      "469,55,Brunt,271\n",
      "470,38,Deanna,160\n",
      "471,52,Brunt,273\n",
      "472,27,Nog,154\n",
      "473,35,Morn,38\n",
      "474,34,Keiko,48\n",
      "475,52,Ben,446\n",
      "476,28,Jean-Luc,378\n",
      "477,50,Gowron,119\n",
      "478,41,Dukat,62\n",
      "479,44,Kasidy,320\n",
      "480,43,Geordi,428\n",
      "481,32,Elim,97\n",
      "482,48,Ben,146\n",
      "483,57,Hugh,99\n",
      "484,22,Leeta,478\n",
      "485,47,Rom,356\n",
      "486,49,Elim,17\n",
      "487,69,Brunt,431\n",
      "488,61,Nog,103\n",
      "489,33,Odo,410\n",
      "490,65,Nerys,101\n",
      "491,60,Rom,2\n",
      "492,19,Dukat,36\n",
      "493,23,Hugh,357\n",
      "494,18,Kasidy,194\n",
      "495,46,Data,155\n",
      "496,39,Gowron,275\n",
      "497,34,Lwaxana,423\n",
      "498,62,Jadzia,36\n",
      "499,62,Leeta,12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = list(bucket.objects.filter(Prefix='data/'))\n",
    "for file in files:\n",
    "    content = file.get()['Body'].read().decode('utf-8') \n",
    "    #print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"abc\": 23, \"efg\": 65}'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = s3_client.get_object(Bucket='yifenghe2019', Key='data/44.txt')\n",
    "txt = obj['Body'].read().decode('utf-8') \n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc': 23, 'efg': 65}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "str1='{\"Python\": \".py\", \"C++\": \".cpp\", \"Java\": \".java\"}'\n",
    "dict1=json.loads(txt)\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '9094F65BC6339E6B',\n",
       "  'HostId': 'soITilSG+oTMWviMhrbWTc8sCzZ31l82NRdMd/RcoNhO18ijGYWH9tBM2wHvRbdGDCH2TAmOShc=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'soITilSG+oTMWviMhrbWTc8sCzZ31l82NRdMd/RcoNhO18ijGYWH9tBM2wHvRbdGDCH2TAmOShc=',\n",
       "   'x-amz-request-id': '9094F65BC6339E6B',\n",
       "   'date': 'Wed, 27 Feb 2019 21:35:12 GMT',\n",
       "   'last-modified': 'Wed, 27 Feb 2019 20:54:16 GMT',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'application/x-directory',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2019, 2, 27, 20, 54, 16, tzinfo=tzutc()),\n",
       " 'ContentLength': 0,\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       " 'ContentType': 'application/x-directory',\n",
       " 'Metadata': {},\n",
       " 'Body': <botocore.response.StreamingBody at 0x7f207846d668>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '26B438F00D891B5A', 'HostId': '0iPN3G1j6cam68u/sf+JBKQ3n3k2e2ZCYZRxfXwS1bd7dMs9XPVx+J44qUMJ2B3StG6qUNQT1eE=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': '0iPN3G1j6cam68u/sf+JBKQ3n3k2e2ZCYZRxfXwS1bd7dMs9XPVx+J44qUMJ2B3StG6qUNQT1eE=', 'x-amz-request-id': '26B438F00D891B5A', 'date': 'Wed, 27 Feb 2019 21:39:23 GMT', 'last-modified': 'Wed, 27 Feb 2019 20:54:37 GMT', 'etag': '\"340f94f5263e12049766b55141dc7ee6\"', 'accept-ranges': 'bytes', 'content-type': 'text/csv', 'content-length': '8277', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'AcceptRanges': 'bytes', 'LastModified': datetime.datetime(2019, 2, 27, 20, 54, 37, tzinfo=tzutc()), 'ContentLength': 8277, 'ETag': '\"340f94f5263e12049766b55141dc7ee6\"', 'ContentType': 'text/csv', 'Metadata': {}, 'Body': <botocore.response.StreamingBody object at 0x7f2078432cf8>}\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3') #low-level functional API\n",
    "obj = s3_client.get_object(Bucket='yifenghe2019', Key='data/friends.csv')\n",
    "print(obj)\n",
    "df_1 = pd.read_csv(obj['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>name</th>\n",
       "      <th>numFriends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>Will</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>Jean-Luc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>Hugh</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>Deanna</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>Quark</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  age      name  numFriends\n",
       "0   0   33      Will         385\n",
       "1   1   26  Jean-Luc           2\n",
       "2   2   55      Hugh         221\n",
       "3   3   40    Deanna         465\n",
       "4   4   68     Quark          21"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={'abc': 23, 'efg': 65}\n",
    "str1=json.dumps(dict1)\n",
    "f=open('33.txt', 'w')\n",
    "f.write(str1)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "file_name_local = 'data/44.txt'\n",
    "bucket='yifenghe2019'\n",
    "s3_file_path = 'data/44.txt'\n",
    "s3_client.upload_file(file_name_local, bucket, s3_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "file_name_local = 'data/4400.txt'\n",
    "bucket='yifenghe2019'\n",
    "s3_file_path = 'data/44.txt'\n",
    "s3_resource = boto3.resource('s3')\n",
    "try:\n",
    "    s3_resource.Bucket(bucket).download_file(s3_file_path, file_name_local)\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "        print(\"The object does not exist.\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset from an online URL into memory, for preprocessing prior to training. This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 927 ms, sys: 299 ms, total: 1.23 s\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "Once the dataset is imported, it's typical as part of the machine learning process to inspect the data, understand the distributions, and determine what type(s) of preprocessing might be needed. You can perform those tasks right here in the notebook. As an example, let's go ahead and look at one of the digits that is part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (2,10)\n",
    "\n",
    "\n",
    "def show_digit(img, caption='', subplot=None):\n",
    "    if subplot==None:\n",
    "        _,(subplot)=plt.subplots(1,1)\n",
    "    imgr=img.reshape((28,28))\n",
    "    subplot.axis('off')\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    plt.title(caption)\n",
    "\n",
    "show_digit(train_set[0][30], 'This is a {}'.format(train_set[1][30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion\n",
    "\n",
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. In this particular case, the Amazon SageMaker implementation of Linear Learner takes recordIO-wrapped protobuf, where the data we have today is a pickle-ized numpy array on disk.\n",
    "\n",
    "Most of the conversion effort is handled by the Amazon SageMaker Python SDK, imported as `sagemaker` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "vectors = np.array([t.tolist() for t in train_set[0]]).astype('float32')\n",
    "labels = np.where(np.array([t.tolist() for t in train_set[1]]) == 0, 1, 0).astype('float32')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training data\n",
    "Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded training data location: s3://yifenghe2019/sagemaker/DEMO-linear-mnist/train/recordio-pb-data\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the linear model\n",
    "\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. Since this data is relatively small, it isn't meant to show off the performance of the Linear Learner training algorithm, although we have tested it on multi-terabyte datasets.\n",
    "\n",
    "Again, we'll use the Amazon SageMaker Python SDK to kick off training, and monitor status until it is completed.  In this example that takes between 7 and 11 minutes.  Despite the dataset being small, provisioning hardware and loading the algorithm container take time upfront.\n",
    "\n",
    "First, let's specify our containers.  Since we want this notebook to run in all 4 of Amazon SageMaker's regions, we'll create a small lookup.  More details on algorithm containers can be found in [AWS documentation](https://docs-aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll kick off the base estimator, making sure to pass in the necessary hyperparameters.  Notice:\n",
    "- `feature_dim` is set to 784, which is the number of pixels in each 28 x 28 image.\n",
    "- `predictor_type` is set to 'binary_classifier' since we are trying to predict whether the image is or is not a 0.\n",
    "- `mini_batch_size` is set to 200.  This value can be tuned for relatively minor improvements in fit and speed, but selecting a reasonable value relative to the dataset is appropriate in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sess)\n",
    "linear.set_hyperparameters(feature_dim=784,\n",
    "                           predictor_type='binary_classifier',\n",
    "                           mini_batch_size=200)\n",
    "\n",
    "linear.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hosting for the model\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint.  This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n",
    "_Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_predictor = linear.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model for use\n",
    "Finally, we can now validate the model for use.  We can pass HTTP POST requests to the endpoint to get back predictions.  To make this easier, we'll again use the Amazon SageMaker Python SDK and specify how to serialize requests and deserialize responses that are specific to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try getting a prediction for a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = linear_predictor.predict(train_set[0][30:31])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, a single prediction works.  We see that for one record our endpoint returned some JSON which contains `predictions`, including the `score` and `predicted_label`.  In this case, `score` will be a continuous value between [0, 1] representing the probability we think the digit is a 0 or not.  `predicted_label` will take a value of either `0` or `1` where (somewhat counterintuitively) `1` denotes that we predict the image is a 0, while `0` denotes that we are predicting the image is not of a 0.\n",
    "\n",
    "Let's do a whole batch of images and evaluate our predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(test_set[0], 100):\n",
    "    result = linear_predictor.predict(array)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(np.where(test_set[1] == 0, 1, 0), predictions, rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the confusion matrix above, we predict 931 images of 0 correctly, while we predict 44 images as 0s that aren't, and miss predicting 49 images of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete the Endpoint\n",
    "\n",
    "If you're ready to be done with this notebook, please run the delete_endpoint line in the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.Session().delete_endpoint(linear_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
